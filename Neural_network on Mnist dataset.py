# -*- coding: utf-8 -*-
"""DM_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qyT3j5SaaIUblpz-skaOvfE99c730-uL
"""

!pip install tensorflow

import matplotlib.pyplot as plt
import tensorflow as tf
mnist=tf.keras.datasets.mnist
(x_train, y_train),(x_test, y_test) = mnist.load_data()


plt.imshow(x_train[0],cmap=plt.cm.binary)
plt.show()
print(y_train[0])


x_train = tf.keras.utils.normalize(x_train, axis=1)
x_test = tf.keras.utils.normalize(x_test, axis=1)

x_input = x_train.reshape((x_train.shape[0], x_train.shape[1]*x_train.shape[2]))
x_output = x_test.reshape((x_test.shape[0],  x_test.shape[1]*x_test.shape[2]))

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(128, activation=tf.nn.sigmoid))
model.add(tf.keras.layers.Dense(128, activation=tf.nn.sigmoid))
model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))
model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])
model.fit(x_input, y_train, epochs=3)

val_loss, val_acc = model.evaluate(x_output, y_test)  # evaluate the out of sample data with model
print(val_loss)  # model's loss (error)
print(val_acc)  # model's accuracy

model.save('output.model')
new_model = tf.keras.models.load_model('output.model')
predictions = new_model.predict(x_output)
#predictions

import numpy as np
for i in range(5):
  print('predicted=',np.argmax(predictions[i]),'actual=',y_test[i])













#!pip install pydrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

from matplotlib import pyplot as plt
import pandas as pd
from copy import deepcopy
import numpy as np

def GoogleDriveAuth(ID):
    auth.authenticate_user()
    gauth = GoogleAuth()
    gauth.credentials = GoogleCredentials.get_application_default()
    drive = GoogleDrive(gauth)
    file = drive.CreateFile({'id': ID})
    return file
File_ID = '1EinH3Q28iwbM9SUceTEiTxz-_KkJXmIh'
file_name = GoogleDriveAuth(File_ID)
file_name.GetContentFile('mnist_train.csv')
train_data = pd.read_csv('mnist_train.csv',delimiter=",",header=None)
train_data = pd.DataFrame(train_data).as_matrix()


File_ID = '1HT80t8kyEn9Fvp80OPfpVb1PlsfN5a6o'
file_name = GoogleDriveAuth(File_ID)
file_name.GetContentFile('mnist_test.csv')
test_data = pd.read_csv('mnist_test.csv',delimiter=",",header=None)
test_data = pd.DataFrame(test_data).as_matrix()

def one_hot(a):
  a=a.reshape(a.shape[0]).astype(int)
  b = np.ones((a.shape[0], different_labels))*0.01
  b[np.arange(a.shape[0]), a] = 0.99
  return b

# %matplotlib inline
import numpy as np
import matplotlib.pyplot as plt

image_size = 28 # width and length
different_labels = 10 #  i.e. 0, 1, 2, 3, ..., 9
image_pixels = image_size * image_size

#normalize the data
norm = 255*0.99 + 0.01
train_imgs = np.asfarray(train_data[:,1:]) / norm  #train input
test_imgs = np.asfarray(test_data[:, 1:]) / norm  #test input
train_labels = np.asfarray(train_data[:, :1])  #train output
test_labels = np.asfarray(test_data[:, :1])  #test output

train_1_hot=one_hot(train_labels)
test_1_hot=one_hot(test_labels)

def initialize_weights():        
        weights_matrices = []        
        layer_index = 1
        no_of_layers = len(structure)
        while layer_index < no_of_layers:
            nodes_in = structure[layer_index-1]
            nodes_out = structure[layer_index]
            wm=np.random.rand(nodes_out,nodes_in+1)*np.sqrt(2/nodes_in)
            weights_matrices.append(wm)
            layer_index += 1
        return weights_matrices

def sigmoid(x):
    return 1 / (1 + np.e ** -x)
activation_function = sigmoid

#Forward and Backward Propogation
def train(input_vector, output_vector):
                   
        #Forward Propogation
        no_of_layers = len(structure)        
        input_vector = np.array(input_vector, ndmin=2).T        
        layer_index = 0        
        in_vectors = [input_vector]    #stores the input vector at each layer 
        
        while layer_index < no_of_layers - 1:
            # adding bias node to the end of the 'input'_vector
            in_vectors[-1] = np.concatenate( (in_vectors[-1],[[1]]) )
            net = np.dot(weights_matrices[layer_index], in_vectors[-1])
            out = activation_function(net)
            in_vectors.append(out)   
            layer_index += 1        
        
        
        
        #Backward Propogation
        layer_index = no_of_layers - 1
        output_vector = np.array(output_vector, ndmin=2).T
         # The input vectors to the various layers
        output_errors = output_vector - out  
        while layer_index > 0:
            out = in_vectors[layer_index]
            ins = in_vectors[layer_index-1]
            if not layer_index==(no_of_layers-1):
                out = out[:-1,:].copy()
            tmp = output_errors * out * (1.0 - out)     
            tmp = np.dot(tmp, ins.T)
                    
            weights_matrices[layer_index-1] += learning_rate * tmp
            
            output_errors = np.dot(weights_matrices[layer_index-1].T,output_errors)
            
            output_errors = output_errors[:-1,:]
            layer_index -= 1

def accuracy(imgs,output):
  crct = 0
  for i in range(len(imgs)):
    predicted= cost_function(imgs[i])
    if predicted == output[i]:
      crct += 1
  print(crct / len(imgs))

def cost_function(image):
  no_of_layers = len(structure)
  # adding bias node to the end of the input_vector
  image = np.concatenate( (image,[1]) )
  ins = np.array(image, ndmin=2).T
  layer_index = 1
  # The input vectors to the various layers
  while layer_index < no_of_layers:
    net = np.dot(weights_matrices[layer_index-1],ins)
    out = activation_function(net)   
    ins = out
    ins = np.concatenate( (ins,[[1]]) )          
    layer_index += 1
  return np.argmax(out)
  #print('Actual=',ans,' Prediction=', np.argmax(out_vector), ' Value',np.max(out_vector))

#initialization
structure=[image_pixels,80,80,different_labels]
learning_rate = 0.1
weights_matrices=initialize_weights()

epochs=3
for epoch in range(epochs):  
  for i in range(len(train_imgs)):
    train(train_imgs[i], train_1_hot[i])
  accuracy(test_imgs,test_labels)

for i in range(5):
  predicted=cost_function(test_imgs[i])
  print('predicted',predicted,'actual',test_labels[i])
accuracy(test_imgs,test_labels)

!pip install pydrive

